{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6caf4d-cc8f-493a-8394-8d424d03b2aa",
   "metadata": {
    "id": "fd6caf4d-cc8f-493a-8394-8d424d03b2aa"
   },
   "source": [
    "# Gerekli Kütüphaneler Yükleniyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe976fb8-03a3-415e-a670-b5a546f3d564",
   "metadata": {
    "id": "fe976fb8-03a3-415e-a670-b5a546f3d564"
   },
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE, RFECV, chi2, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, RocCurveDisplay, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cgkvsMuW0DeU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgkvsMuW0DeU",
    "outputId": "609c9fff-3916-4264-fe67-b74ff15b2af5"
   },
   "outputs": [],
   "source": [
    "n_cpu = os.cpu_count()\n",
    "print(\"Number of CPUs in the system:\", n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107e0fb-a0ef-4d97-b59c-5635fd0ca96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUM_OF_FEATURES=int(input(\"Number of Features. If you want to continue without feature selection, enter 0.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c8af6c-69a1-4e94-a394-3a19edbd1ac1",
   "metadata": {
    "id": "67c8af6c-69a1-4e94-a394-3a19edbd1ac1"
   },
   "source": [
    "# Veriler okunuyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be30cf-0d6e-4cbd-b47f-aca419acbf41",
   "metadata": {
    "id": "a7be30cf-0d6e-4cbd-b47f-aca419acbf41"
   },
   "outputs": [],
   "source": [
    "anaKlasor = \"C:\\\\Users\\\\cihan.aksop\\\\Desktop\\\\\"\n",
    "\n",
    "X_train = pd.read_csv(os.path.join(anaKlasor, 'X_train.txt'), sep='\\\\s+', header=None)\n",
    "X_test = pd.read_csv(os.path.join(anaKlasor, 'X_test.txt'), sep='\\\\s+', header=None)\n",
    "y_train = pd.read_csv(os.path.join(anaKlasor, 'y_train.txt'), sep='\\\\s+', header=None)\n",
    "y_test = pd.read_csv(os.path.join(anaKlasor, 'y_test.txt'), sep='\\\\s+', header=None)\n",
    "print(\"Data Loaded Successfully\")\n",
    "print(f\"Shape of train data: {X_train.shape}\")\n",
    "print(f\"Shape of test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f2895b-16a4-4fc6-b1b3-b072b92e0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and testing data for analysis\n",
    "X = pd.concat([X_train, X_test], ignore_index=True)\n",
    "y = pd.concat([y_train, y_test], ignore_index=True)\n",
    "y.columns=[\"activity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57930d2-dc85-4937-8bed-b4da49da2f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load activity labels\n",
    "activity_labels = pd.read_csv(os.path.join(anaKlasor, 'activity_labels.txt'), sep=' ', header=None, names=['id', 'activity_name'])\n",
    "y['activity_name'] = y['activity'].map(activity_labels.set_index('id')['activity_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4414794d-1cf6-4d9c-b7a4-79dad6476bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f697897-f461-4ed2-822d-dd0e233bafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601cf2c1-b4f6-4983-aac7-63220da77711",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of missing values\")\n",
    "print(X_train.isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a92462-2961-49d9-8e8a-1a8f786596b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data types:\")\n",
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8450df5-160c-444a-8db7-beed251018fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated columns\n",
    "tekil_deger_sayisi = X_train.nunique()\n",
    "tekil_degerli_sutunlar = tekil_deger_sayisi[tekil_deger_sayisi == 1].index\n",
    "print(\"Number of Duplicated Columns: \" + str(len(tekil_degerli_sutunlar)))\n",
    "X_train.drop(tekil_degerli_sutunlar, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f2f45-b252-4ba8-ac48-7bc46b1e6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated rows\n",
    "X_train.drop_duplicates(inplace = True)\n",
    "X_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e755f-44ae-437b-bf91-67b7435f105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the new dataset:{X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5b123-b8be-4b01-b74b-cca8d7e1cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequencies of y train values\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ae192-0a97-4e51-8cab-ec6b6a56c01d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "414ae192-0a97-4e51-8cab-ec6b6a56c01d",
    "outputId": "bf1fc829-e1ee-4079-c277-0fd4de9ae90b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Değişkenler arasındaki korelasyon matrisi çizdiriliyor\n",
    "pd.concat([X_train, y_train]).corr().style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac942c-8886-49a4-ae34-39a5952ab320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fft_features(X):\n",
    "    fft_features = np.abs(fft(X, axis=1))\n",
    "    return np.hstack((X, fft_features))\n",
    "#X_train = add_fft_features(X_train)\n",
    "#X_test = add_fft_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df8601-b9a9-457c-940c-f16dc150ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y['activity'])\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "selected_features = [f'Feature {i}' for i in selected_feature_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8115002b-e3f3-4837-b249-5d10a988c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add selected features back to a DataFrame for visualization\n",
    "X_selected_df = pd.DataFrame(X_selected, columns=selected_features)\n",
    "X_selected_df['activity_name'] = y['activity_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba49d589-3258-4953-a13f-96c6f3c1e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data size after feature selection: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5b04f-62dc-4668-a20b-29e06cf3af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Activity Distribution\n",
    "activity_distribution = y['activity_name'].value_counts(normalize=True) * 100\n",
    "plt.figure(figsize=(8, 5))\n",
    "activity_distribution.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Activity Distribution\")\n",
    "plt.xlabel(\"Activity\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6847790-3bf3-4a56-9d3c-dd76fe01d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Correlation Matrix for Selected Features\n",
    "correlation_matrix = X_selected_df[selected_features].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix of Selected Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26870b-af47-4b71-90f8-24128cb61afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pairplot of Selected Features by Activity\n",
    "sns.pairplot(X_selected_df, vars=selected_features[:3], hue=\"activity_name\", palette=\"Set2\")\n",
    "plt.suptitle(\"Pairplot of Selected Features by Activity\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841c49e-ef5f-44cc-ab7c-3a8a8a9d10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Trends (Selected Features)\n",
    "plt.figure(figsize=(8, 5))\n",
    "for feature in selected_features[:3]:\n",
    "    for activity in X_selected_df['activity_name'].unique():\n",
    "        subset = X_selected_df[X_selected_df['activity_name'] == activity].head(50)\n",
    "        plt.plot(subset.index, subset[feature].rolling(5).mean(), label=f\"{activity} ({feature})\", alpha=0.7)\n",
    "\n",
    "plt.title(\"Feature Trends: Smoothed Selected Features\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Feature Value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6b9bb-4aef-4fe5-b397-4435792bc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Clustering Example with Selected Features\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "X_selected_df['cluster'] = kmeans.fit_predict(X_selected_df[selected_features])\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(\n",
    "    x=selected_features[0], y=selected_features[1],\n",
    "    hue=\"cluster\", palette=\"viridis\", data=X_selected_df, legend=\"full\")\n",
    "plt.title(\"Clustering Results Based on Selected Features\")\n",
    "plt.xlabel(selected_features[0])\n",
    "plt.ylabel(selected_features[1])\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3d1a3-f332-4818-b77e-dd3997bd5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feature Importance Visualization\n",
    "feature_scores = selector.scores_[selected_feature_indices]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(selected_features, feature_scores, color=\"skyblue\")\n",
    "plt.title(\"Feature Importance Based on ANOVA F-Test\")\n",
    "plt.xlabel(\"F-Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db82b669-1e9e-469b-9c1b-b7119c203ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Boxplot for Each Selected Feature by Activity\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, feature in enumerate(selected_features[:3]):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.boxplot(x=\"activity_name\", y=feature, data=X_selected_df, palette=\"Set3\")\n",
    "    plt.title(f\"{feature} Distribution by Activity\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5441940-f6dd-4029-900a-b38ff2472822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Histogram of Selected Features\n",
    "plt.figure(figsize=(10, 6))\n",
    "for feature in selected_features[:3]:\n",
    "    sns.histplot(X_selected_df[feature], kde=True, label=feature, alpha=0.7)\n",
    "plt.title(\"Histogram of Selected Features\")\n",
    "plt.xlabel(\"Feature Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e312c88f-623a-4a1c-be63-76e4d222384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test, y_train, y_test, sample_size=100000):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    X_train, y_train = resample(X_train, y_train, n_samples=100000, random_state=42)\n",
    "    y_train = y_train.values.ravel()\n",
    "    y_test = y_test.values.ravel()\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_test = encoder.transform(y_test)\n",
    "    print(f\"Data Preprocessing Completed. Training samples: {len(X_train)}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4135b8-d385-4f38-861b-c1bd8bee8011",
   "metadata": {
    "id": "6e4135b8-d385-4f38-861b-c1bd8bee8011"
   },
   "source": [
    "# Makine Öğrenmesi Algoritmaları Uygulanıyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f201a6-8f3a-4b2d-9c7f-ddd3334a6d69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "d2f201a6-8f3a-4b2d-9c7f-ddd3334a6d69",
    "outputId": "b374d0dc-e385-4e44-b806-708a19017fe1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Makine öğrenmesi yöntemleri ve parametreleri belirleniyor\n",
    "accuracies = {}\n",
    "parameter_group = {}\n",
    "parameter_group[\"Decision Tree\"] = {'function': DecisionTreeClassifier(random_state=1),\n",
    "                                    'parameters': {'criterion': [\"entropy\", \"log_loss\"],\n",
    "                                                  'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                                                  'ccp_alpha': [.001, .0001, .00001],\n",
    "                                                  'max_depth' : [5, 10, 15],}}\n",
    "\n",
    "parameter_group[\"K-Neightbors\"] = {'function': KNeighborsClassifier(),\n",
    "                                   'parameters': {'n_neighbors': [2, 5],\n",
    "                                                  'weights': [\"uniform\", \"distance\"],\n",
    "                                                  'algorithm': [\"ball_tree\", \"kd_tree\"],\n",
    "                                                  'metric': ['euclidean', 'manhattan'],\n",
    "                                                  'p': [1]}}\n",
    "parameter_group[\"SVC\"] = {'function': SVC(),\n",
    "                          'parameters': {'C': [1, 10],\n",
    "                                         'kernel': [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "                                         'degree': [5, 7, 10]}}\n",
    "parameter_group[\"Random Forest\"] = {'function': RandomForestClassifier(),\n",
    "                                    'parameters': {'n_estimators': [5, 7, 10],\n",
    "                                                   'criterion': [\"gini\", \"entropy\", \"log_loss\"],\n",
    "                                                   'max_features': [\"sqrt\", \"log2\"]}}\n",
    "parameter_group[\"Extra Trees\"] = {'function': ExtraTreesClassifier(),\n",
    "                                  'parameters': {'criterion': [\"gini\", \"entropy\", \"log_loss\"],\n",
    "                                                 'max_features': [\"sqrt\", \"log2\"]}}\n",
    "\n",
    "parameter_group[\"XGB\"] = {'function': xgb.XGBClassifier(objective='multi:softmax', seed=42, num_class = len(np.unique(y_train)), max_depth=3),\n",
    "                          'parameters': {'min_child_weight': [1, 3],\n",
    "                                         'gamma': [0.5, 1],\n",
    "                                         'subsample': [0.6, 0.8],\n",
    "                                         'colsample_bytree': [0.6, 0.8]}}\n",
    "\n",
    "parameter_group[\"LGBM\"] = {'function': lgb.LGBMClassifier(objective = 'multiclass', boosting_type = \"dart\", num_class= len(np.unique(y_train)), verbose = 0, random_state=42),\n",
    "                           'parameters': {'learning_rate': [0.01, .1],\n",
    "                                          'n_estimators': [8,16],\n",
    "                                          'reg_lambda' : [1,1.2]}}\n",
    "\n",
    "parameter_group[\"Logistic\"] = {'function': LogisticRegression(),\n",
    "                               'parameters': {\"penalty\":[\"l2\"]}}\n",
    "\n",
    "#parameter_group[\"Gradient Boosting\"] = {'function': GradientBoostingClassifier(random_state=42),\n",
    "#                                        'parameters': {\"learning_rate\": [0.01, 0.1],\n",
    "#                                                        \"max_depth\":[3,8]\n",
    "#                                                        }}\n",
    "\n",
    "for method in parameter_group:\n",
    "    print(method)\n",
    "    classifier = parameter_group[method][\"function\"]\n",
    "    model = GridSearchCV(classifier, parameter_group[method][\"parameters\"], n_jobs=-1, verbose=1, cv=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(model.best_params_)\n",
    "\n",
    "    y_predict = model.predict(X_train)\n",
    "    accuracy = accuracy_score(y_train, y_predict)\n",
    "    conf_matrix = confusion_matrix(y_train, y_predict)\n",
    "\n",
    "    print('TRAIN RESULTS')\n",
    "    print('Train Accuracy: {:.3f}'.format(accuracy))\n",
    "    print(classification_report(y_train, y_predict))\n",
    "    print('Confusion matrix:')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    y_predict = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    conf_matrix = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "    print('TEST RESULTS')\n",
    "    print('Test Accuracy: {:.3f}'.format(accuracy))\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    print('Confusion matrix:')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    accuracies[method] = {'model':  model.best_estimator_,\n",
    "                                   'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b093c3-4ebb-4aac-8782-2c9841c14d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342fe89-d4b4-413b-919a-0bfaff8e1d82",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a694f-638b-485c-ba5a-2d0105d21f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTM_MulticlassClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LSTM_MulticlassClassifier, self).__init__()\n",
    "        self.fc1 = nn.LSTM(input_size, 64, 3, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,_ = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "model = LSTM_MulticlassClassifier(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracies[\"LSTM\"] = {'model':  model,\n",
    "                                   'accuracy': correct / total}\n",
    "    print(f'Accuracy of the network on the test data: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b89526-9d09-4da8-acdb-73e122c8449a",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29ba18-978a-41fd-9eaf-ad34d3a860ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_MulticlassClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(GRU_MulticlassClassifier, self).__init__()\n",
    "        self.fc1 = nn.GRU(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,_ = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "model = GRU_MulticlassClassifier(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracies[\"GRU\"] = {'model':  model,\n",
    "                                   'accuracy': correct / total}\n",
    "    print(f'Accuracy of the network on the test data: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993407c-6e70-4ad1-9280-0235e169a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_MulticlassClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(BiLSTM_MulticlassClassifier, self).__init__()\n",
    "        self.fc1 = nn.LSTM(input_size, 64, 3, bidirectional=True, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,_ = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "model = BiLSTM_MulticlassClassifier(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracies[\"BiLSTM\"] = {'model':  model,\n",
    "                                   'accuracy': correct / total}\n",
    "    print(f'Accuracy of the network on the test data: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f7c05-2d24-408d-98a6-fac05fd6dd56",
   "metadata": {},
   "source": [
    "# Meta Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f75581-9939-485b-97e6-8e79a61507b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traditional_train = np.c_[accuracies[\"Decision Tree\"][\"model\"].predict(X_train).T, \n",
    "          accuracies[\"Random Forest\"][\"model\"].predict(X_train).T,\n",
    "          accuracies[\"Extra Trees\"][\"model\"].predict(X_train).T,\n",
    "          accuracies[\"XGB\"][\"model\"].predict(X_train).T,\n",
    "          accuracies[\"LGBM\"][\"model\"].predict(X_train).T,\n",
    "          accuracies[\"Logistic\"][\"model\"].predict(X_train).T]\n",
    "\n",
    "X_neural_train = np.c_[torch.max(accuracies[\"LSTM\"][\"model\"](X_train).data, 1).indices.numpy().T,\n",
    "                torch.max(accuracies[\"GRU\"][\"model\"](X_train).data, 1).indices.numpy().T,\n",
    "                torch.max(accuracies[\"BiLSTM\"][\"model\"](X_train).data, 1).indices.numpy().T]\n",
    "\n",
    "X_traditional_test = np.c_[accuracies[\"Decision Tree\"][\"model\"].predict(X_test).T, \n",
    "          accuracies[\"Random Forest\"][\"model\"].predict(X_test).T,\n",
    "          accuracies[\"Extra Trees\"][\"model\"].predict(X_test).T,\n",
    "          accuracies[\"XGB\"][\"model\"].predict(X_test).T,\n",
    "          accuracies[\"LGBM\"][\"model\"].predict(X_test).T,\n",
    "          accuracies[\"Logistic\"][\"model\"].predict(X_test).T]\n",
    "\n",
    "X_neural_test = np.c_[torch.max(accuracies[\"LSTM\"][\"model\"](X_test).data, 1).indices.numpy().T,\n",
    "                torch.max(accuracies[\"GRU\"][\"model\"](X_test).data, 1).indices.numpy().T,\n",
    "                torch.max(accuracies[\"BiLSTM\"][\"model\"](X_test).data, 1).indices.numpy().T]\n",
    "\n",
    "X_meta_train = np.c_[X_traditional_train, X_neural_train]\n",
    "X_meta_test = np.c_[X_traditional_test, X_neural_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13022384-2cd1-41fb-9fd5-2bb1a399783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
    "meta_learner.fit(X_meta_train, y_train)\n",
    "\n",
    "y_predict = meta_learner.predict(X_meta_train)\n",
    "accuracy = accuracy_score(y_train, y_predict)\n",
    "conf_matrix = confusion_matrix(y_train, y_predict)\n",
    "\n",
    "print('TRAIN RESULTS')\n",
    "print('Train Accuracy: {:.3f}'.format(accuracy))\n",
    "print(classification_report(y_train, y_predict))\n",
    "print('Confusion matrix:')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "y_predict = meta_learner.predict(X_meta_test)\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "conf_matrix = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "print('TEST RESULTS')\n",
    "print('Test Accuracy: {:.3f}'.format(accuracy))\n",
    "print(classification_report(y_test, y_predict))\n",
    "print('Confusion matrix:')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c332cc-2e09-4c5e-a1f1-e1bc9b91149f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
